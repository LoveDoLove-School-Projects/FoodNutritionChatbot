{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXnsRU00Fil1"
   },
   "source": [
    "# Init Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1745573948625,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "MuOUG8jGGfZ2",
    "outputId": "33495c3b-f945-4fc3-842e-a671da8a21ab"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For nvidia GPU (AMD and Intel do not ned this step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1745574033059,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "FOSQbrkcguRF",
    "outputId": "1f2beb0b-2077-4de5-e82e-436674e0faed"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiC--DolvWFm"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-cpu==2.10\n",
    "!pip install tensorflow-directml-plugin\n",
    "!pip install \"numpy<2.0\"\n",
    "!pip install pandas\n",
    "!pip install joblib\n",
    "!pip install scikit-learn\n",
    "!pip install fuzzywuzzy\n",
    "!pip install nltk\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install tabulate\n",
    "!pip install rouge_score\n",
    "!pip install pyyaml\n",
    "!pip install pulp\n",
    "!pip install ipywidgets\n",
    "!pip install python-Levenshtein\n",
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R6SNiVc_WGRZ"
   },
   "source": [
    "## Install nltk packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2489,
     "status": "ok",
     "timestamp": 1745574313595,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "8SzCUyuUvYGO",
    "outputId": "05cf72ba-72fb-47a3-8d8a-ab2c69d440fb"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wp3KXP7yWGRa"
   },
   "source": [
    "## Check tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2748,
     "status": "ok",
     "timestamp": 1745574318214,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "s84SsQVIWGRa",
    "outputId": "400a5a14-e39c-46dc-f2e3-fb9d75a62d97"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"âœ… TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check tensorflow devices checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSNEgGxIbHCR"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nadmun9bJjM"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJUIt1s2aDFj"
   },
   "source": [
    "# Clean Data (V2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOcfpB5kaGLU"
   },
   "source": [
    "## Load and Inspect Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1745575813564,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "d69fZzlwaE9l",
    "outputId": "21a71c9b-a695-4eaa-de19-5c9a816c700f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"food_data_dataset.csv\")\n",
    "\n",
    "# Inspect structure\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(\"Missing values:\\n\", df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_NwPJbfaIRU"
   },
   "source": [
    "## Basic Cleaning & Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745575815869,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "gTkLkojmaKuL"
   },
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "for col in df.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        df[col] = df[col].fillna(df[col].mean())  # numeric: mean\n",
    "    else:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])  # categorical: mode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52VMJx4kaMcp"
   },
   "source": [
    "## Outlier Clipping (IQR Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 80,
     "status": "ok",
     "timestamp": 1745575817979,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "TlQQPK6baMot"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=np.number).columns\n",
    "for col in numerical_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    df[col] = df[col].clip(lower, upper)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6nlry7YaRYm"
   },
   "source": [
    "## Normalize Nutrients (Optional, but recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745575819184,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "HjYcSaTraUNH"
   },
   "outputs": [],
   "source": [
    "if 'Serving_Size_g' in df.columns:\n",
    "    df[numerical_cols] = df[numerical_cols].div(df['Serving_Size_g'], axis=0) * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOxUNvjraVmV"
   },
   "source": [
    "## Clean the Food_Item Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1745575820397,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "5gYhq6jKaYRG"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize spaces\n",
    "    return text\n",
    "\n",
    "df['Food_Item'] = df['Food_Item'].astype(str).apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QUJA9jImaacp"
   },
   "source": [
    "## Label Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1745575821650,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "y2jT9wjbaas1"
   },
   "outputs": [],
   "source": [
    "# Convert Nutrition_Density to categories\n",
    "df['Meal_Type'] = pd.qcut(df['Nutrition_Density'], q=3, labels=['Low', 'Medium', 'High'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ycix0UBUaenC"
   },
   "source": [
    "## TF-IDF Vector for Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48,
     "status": "ok",
     "timestamp": 1745575822808,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "IS5jILA9ae7d",
    "outputId": "49d6480a-e08f-48b5-e2e6-07b1acb68e44"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_text = tfidf.fit_transform(df['Food_Item']).toarray()\n",
    "joblib.dump(tfidf, \"tfidf_vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK8f-UnUakZt"
   },
   "source": [
    "## Standardize Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1745575823972,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "V_cEJhw_ak_k",
    "outputId": "a70e862b-ae01-45a2-c4f3-68f7532444c4"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numeric_cols = [col for col in df.columns if col not in ['Food_Item', 'Meal_Type']]\n",
    "df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_num = scaler.fit_transform(df[numeric_cols])\n",
    "joblib.dump(scaler, \"nutrition_scaler.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vIvbARKanXt"
   },
   "source": [
    "## Combine Text + Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745575825151,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "q62x6D0Kan5e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.hstack([X_text, X_num])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rU-tBlj-ated"
   },
   "source": [
    "## Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1745575826340,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "C1ta9KoSavnl",
    "outputId": "144bcf2a-44bc-49fb-c842-c6fb74fda8ed"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Meal_Type'])\n",
    "joblib.dump(label_encoder, \"label_encoder.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-y82A5h0ayGQ"
   },
   "source": [
    "## Stratified Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1745575827617,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "CDwIBbLXazqg"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_idx, test_idx in split.split(X, y):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2UZeUsda2ic"
   },
   "source": [
    "## Save Cleaned Dataset (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1745575829342,
     "user": {
      "displayName": "JUN XIANG CHONG",
      "userId": "14439078054689888844"
     },
     "user_tz": -480
    },
    "id": "uTa9zlyHa38L"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_food_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP_LYuGPZdr8"
   },
   "source": [
    "# Train Data With Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Data With Tensorflow (V17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import yaml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, log_loss\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint,\n",
    "    ReduceLROnPlateau, TensorBoard\n",
    ")\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG_PATH = \"config.yaml\"\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "def load_config(path):\n",
    "    if os.path.exists(path):\n",
    "        return yaml.safe_load(open(path))\n",
    "    return {\n",
    "        'data_path': 'cleaned_food_data.csv',\n",
    "        'text_column': 'Food_Item',\n",
    "        'target_column': 'Meal_Type',\n",
    "        'numeric_columns': [],\n",
    "        'tfidf__ngram_range': (1,2),\n",
    "        'tfidf__max_features': 1500,\n",
    "        'validation_split': 0.2,\n",
    "        'model': {\n",
    "            'layers': [1024, 512, 256],\n",
    "            'dropouts': [0.4, 0.3, 0.2],\n",
    "            'l2': 0.001,\n",
    "            'learning_rate': 1e-3,\n",
    "            'batch_size': 32,\n",
    "            'epochs': 100\n",
    "        },\n",
    "        'cv_splits': 5,\n",
    "        'grid_search': False,\n",
    "        'gen_refs_path': 'generation_refs.json',\n",
    "        'gen_hyps_path': 'generation_hyps.json',\n",
    "        'feedback_path': 'user_feedback.csv'\n",
    "    }\n",
    "\n",
    "config = load_config(CONFIG_PATH)\n",
    "\n",
    "# === Data Loading & Cleaning ===\n",
    "logging.info(\"Loading and cleaning data...\")\n",
    "\n",
    "def load_and_clean(path, text_col):\n",
    "    df = pd.read_csv(path)\n",
    "    df[text_col] = (\n",
    "        df[text_col].astype(str)\n",
    "        .str.lower()\n",
    "        .str.replace(r\"\\(.*?\\)\", \"\", regex=True)\n",
    "        .str.replace(r\"[^a-zA-Z\\s]\", \"\", regex=True)\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        .str.strip()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Load and clean\n",
    "df = load_and_clean(config['data_path'], config['text_column'])\n",
    "\n",
    "# === Label Creation ===\n",
    "logging.info(\"Creating quantile-based labels...\")\n",
    "labels = ['Low', 'Medium', 'High']\n",
    "df[config['target_column']] = pd.qcut(\n",
    "    df['Nutrition_Density'], q=3, labels=labels\n",
    ")\n",
    "\n",
    "# Fill numeric missing\n",
    "num_cols = [c for c in df.columns if c not in [config['text_column'], config['target_column']]]\n",
    "df[num_cols] = df[num_cols].fillna(0)\n",
    "config['numeric_columns'] = num_cols\n",
    "\n",
    "# Encode targets\n",
    "label_enc = LabelEncoder()\n",
    "y = label_enc.fit_transform(df[config['target_column']])\n",
    "joblib.dump(label_enc, 'label_encoder.pkl')\n",
    "\n",
    "# Preprocessing pipelines\n",
    "text_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        ngram_range=config['tfidf__ngram_range'],\n",
    "        max_features=config['tfidf__max_features']\n",
    "    ))\n",
    "])\n",
    "num_pipeline = Pipeline([('scaler', StandardScaler())])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('text', text_pipeline, config['text_column']),\n",
    "    ('num', num_pipeline, num_cols)\n",
    "], sparse_threshold=0)\n",
    "\n",
    "# Build Keras model\n",
    "def build_model(input_dim, cfg):\n",
    "    inp = Input(shape=(input_dim,))\n",
    "    x = inp\n",
    "    for units, drop in zip(cfg['layers'], cfg['dropouts']):\n",
    "        x = Dense(units,\n",
    "                  activation='relu',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(cfg['l2']))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(drop)(x)\n",
    "    out = Dense(len(label_enc.classes_), activation='softmax')(x)\n",
    "    model = Model(inp, out)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(cfg['learning_rate']),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Assemble full pipeline\n",
    "clf = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('keras', KerasClassifier(\n",
    "        build_fn=lambda: build_model(\n",
    "            input_dim=preprocessor.fit_transform(df).shape[1],\n",
    "            cfg=config['model']\n",
    "        ),\n",
    "        epochs=config['model']['epochs'],\n",
    "        batch_size=config['model']['batch_size'],\n",
    "        validation_split=config['validation_split'],\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            EarlyStopping('val_loss', patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau('val_loss', factor=0.5, patience=4, min_lr=1e-6),\n",
    "            ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True),\n",
    "            TensorBoard(log_dir=f\"logs/v17_{datetime.now():%Y%m%d_%H%M%S}\")\n",
    "        ]\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Optional grid search\n",
    "if config['grid_search']:\n",
    "    param_grid = {\n",
    "        'keras__epochs': [50, 100],\n",
    "        'keras__batch_size': [32, 64]\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=config['cv_splits'], shuffle=True, random_state=SEED)\n",
    "    clf = GridSearchCV(clf, param_grid, cv=cv, scoring='accuracy')\n",
    "\n",
    "# Train/test split\n",
    "df_features = df.copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_features, y, test_size=0.2, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "# Training\n",
    "logging.info(\"Training Version 17 model...\")\n",
    "clf.fit(X_train, y_train)\n",
    "best = clf.best_estimator_ if hasattr(clf, 'best_estimator_') else clf\n",
    "\n",
    "# Predictions & Evaluation\n",
    "X_t = best.named_steps['preproc'].transform(X_test)\n",
    "y_probs = best.named_steps['keras'].model.predict(X_t)\n",
    "y_pred = np.argmax(y_probs, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"===== Classification Report =====\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_enc.classes_))\n",
    "\n",
    "# Per-class metrics\n",
    "prf = precision_recall_fscore_support(y_test, y_pred, zero_division=0)\n",
    "df_metrics = pd.DataFrame({\n",
    "    'precision': prf[0],\n",
    "    'recall': prf[1],\n",
    "    'f1_score': prf[2],\n",
    "    'support': prf[3]\n",
    "}, index=label_enc.classes_)\n",
    "print(\"\\n===== Per-Class Metrics =====\")\n",
    "print(df_metrics)\n",
    "\n",
    "# Overall metrics\n",
    "overall_acc = accuracy_score(y_test, y_pred) * 100\n",
    "overall_prec = precision_score(y_test, y_pred, average='macro', zero_division=0) * 100\n",
    "overall_rec = recall_score(y_test, y_pred, average='macro', zero_division=0) * 100\n",
    "overall_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0) * 100\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {overall_acc:.2f}%\")\n",
    "print(f\"Overall Precision (macro): {overall_prec:.2f}%\")\n",
    "print(f\"Overall Recall (macro): {overall_rec:.2f}%\")\n",
    "print(f\"Overall F1-score (macro): {overall_f1:.2f}%\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_probs):.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc_score(tf.keras.utils.to_categorical(y_test), y_probs, multi_class='ovr'):.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d',\n",
    "            xticklabels=label_enc.classes_,\n",
    "            yticklabels=label_enc.classes_)\n",
    "plt.title('Confusion Matrix v17')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# === Visualization Updates v17 ===\n",
    "# 1) Violin plot of predicted probabilities by true class\n",
    "prob_df = pd.DataFrame(y_probs, columns=label_enc.classes_)\n",
    "prob_df['true_class'] = label_enc.inverse_transform(y_test)\n",
    "prob_melt = pd.melt(\n",
    "    prob_df,\n",
    "    id_vars='true_class',\n",
    "    var_name='predicted_class',\n",
    "    value_name='probability'\n",
    ")\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.violinplot(\n",
    "    x='predicted_class',\n",
    "    y='probability',\n",
    "    hue='true_class',\n",
    "    data=prob_melt,\n",
    "    split=True\n",
    ")\n",
    "plt.title('Predicted Probability Distributions by Class (v17)')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.legend(title='True Class')\n",
    "plt.savefig('probability_violin.png')\n",
    "plt.close()\n",
    "\n",
    "# 2) Calibration curve per class\n",
    "plt.figure(figsize=(8, 6))\n",
    "for idx, cls in enumerate(label_enc.classes_):\n",
    "    true_bin = (y_test == idx).astype(int)\n",
    "    prob_cls = y_probs[:, idx]\n",
    "    prob_true, prob_pred = calibration_curve(true_bin, prob_cls, n_bins=10)\n",
    "    plt.plot(prob_pred, prob_true, marker='o', label=cls)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.title('Calibration Curve by Class (v17)')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.legend()\n",
    "plt.savefig('calibration_curve.png')\n",
    "plt.close()\n",
    "\n",
    "# Generation metrics (BLEU, ROUGE)\n",
    "if os.path.exists(config['gen_refs_path']) and os.path.exists(config['gen_hyps_path']):\n",
    "    with open(config['gen_refs_path']) as rf, open(config['gen_hyps_path']) as hf:\n",
    "        refs = json.load(rf)\n",
    "        hyps = json.load(hf)\n",
    "    refs_tok = [[r.split() for r in rlist] for rlist in refs]\n",
    "    hyps_tok = [h.split() for h in hyps]\n",
    "    bleu = corpus_bleu(refs_tok, hyps_tok, smoothing_function=SmoothingFunction().method4)\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)\n",
    "    r1=[]; rL=[]\n",
    "    for rlist, hyp in zip(refs, hyps):\n",
    "        sc = scorer.score(\" \".join(rlist), hyp)\n",
    "        r1.append(sc['rouge1'].fmeasure)\n",
    "        rL.append(sc['rougeL'].fmeasure)\n",
    "    print(f\"BLEU: {bleu:.4f}, ROUGE-1 F1: {np.mean(r1):.4f}, ROUGE-L F1: {np.mean(rL):.4f}\")\n",
    "else:\n",
    "    logging.warning(\"Skipping generation metrics.\")\n",
    "\n",
    "# Feedback analysis\n",
    "if os.path.exists(config['feedback_path']):\n",
    "    fb = pd.read_csv(config['feedback_path'])\n",
    "    for col in ['usability_score', 'satisfaction_score']:\n",
    "        if col in fb:\n",
    "            print(f\"{col}: mean={fb[col].mean():.2f}, median={fb[col].median():.2f}\")\n",
    "else:\n",
    "    logging.warning(\"Skipping feedback analysis.\")\n",
    "\n",
    "# Save artifacts\n",
    "logging.info(\"Saving V17 artifacts...\")\n",
    "best.named_steps['keras'].model.save('food_nutrition_model.keras')\n",
    "joblib.dump(label_enc.classes_.tolist(), 'class_names.pkl')\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "with open('version_info.json', 'w') as vf:\n",
    "    json.dump({'version': '17', 'timestamp': str(datetime.now())}, vf)\n",
    "logging.info(\"âœ… Version 17 pipeline complete.\")\n",
    "\n",
    "# References:\n",
    "# - Pandas: https://pandas.pydata.org/\n",
    "# - NumPy: https://numpy.org/\n",
    "# - Scikit-learn: https://scikit-learn.org/stable/\n",
    "# - TensorFlow Keras: https://www.tensorflow.org/guide/keras/\n",
    "# - Seaborn Violin: https://seaborn.pydata.org/generated/seaborn.violinplot.html\n",
    "# - Calibration Curve: https://scikit-learn.org/stable/modules/calibration.html\n",
    "# - Matplotlib: https://matplotlib.org/\n",
    "# - NLTK BLEU: https://www.nltk.org/api/nltk.translate.html\n",
    "# - rouge_score: https://pypi.org/project/rouge-score/\n",
    "# - joblib: https://joblib.readthedocs.io/\n",
    "# - PyYAML: https://pyyaml.org/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AyBfUW7jmFg"
   },
   "source": [
    "# ChatBot Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot (V21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import logging\n",
    "import difflib\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "from rich.console import Console\n",
    "from rich.spinner import Spinner\n",
    "from rich.text import Text\n",
    "from autocorrect import Speller\n",
    "\n",
    "# === NLTK Resources ===\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# === Logging Setup ===\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "console = Console()\n",
    "\n",
    "# === Autocorrect Setup ===\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "# === Paths & Artifacts ===\n",
    "MODEL_PATH        = 'food_nutrition_model.keras'\n",
    "PREPROCESSOR_PATH = 'preprocessor.pkl'\n",
    "LABELENC_PATH     = 'label_encoder.pkl'\n",
    "DATA_PATH         = 'cleaned_food_data.csv'\n",
    "\n",
    "# === Load model and data ===\n",
    "with console.status(\"Loading model and data...\", spinner=\"dots\"):\n",
    "    model     = load_model(MODEL_PATH)\n",
    "    preproc   = joblib.load(PREPROCESSOR_PATH)\n",
    "    label_enc = joblib.load(LABELENC_PATH)\n",
    "    data      = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# === Small-talk Patterns & Responses ===\n",
    "GREETINGS_PATTERN = re.compile(r\"\\b(hi|hello|hey|how are you|good morning|good afternoon|good evening)\\b\", re.I)\n",
    "THANKS_PATTERN    = re.compile(r\"\\b(thanks|thank you|thx)\\b\", re.I)\n",
    "FAREWELL_PATTERN  = re.compile(r\"\\b(bye|goodbye|see you|farewell)\\b\", re.I)\n",
    "\n",
    "GREETING_RESPONSES = [\n",
    "    \"Hello there! ðŸ¥¦ How can I help you with your nutrition today?\",\n",
    "    \"Hi! I'm here to chat about food and nutrition. What would you like to know?\",\n",
    "    \"Hey! Want to know the meal type or nutrition facts of something?\"\n",
    "]\n",
    "THANK_RESPONSES = [\n",
    "    \"You're welcome! ðŸ½ï¸\",\n",
    "    \"Anytime! Let me know if you have more questions.\",\n",
    "    \"Glad to help!\"\n",
    "]\n",
    "FAREWELL_RESPONSES = [\n",
    "    \"Goodbye! Stay healthy! ðŸŒ±\",\n",
    "    \"See you later! Keep eating well!\",\n",
    "    \"Farewell! Remember to balance your meals.\"\n",
    "]\n",
    "\n",
    "# === Prepare food lookup ===\n",
    "data['food_lower'] = data['Food_Item'].str.lower()\n",
    "food_list = data['food_lower'].tolist()\n",
    "food_map  = {row['food_lower']: row.drop('food_lower').to_dict() for _, row in data.iterrows()}\n",
    "vectorizer = TfidfVectorizer().fit(food_list)\n",
    "vectors    = vectorizer.transform(food_list)\n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "# === Utility Functions ===\n",
    "def get_synonyms(word):\n",
    "    return {lm.name().lower() for syn in wordnet.synsets(word) for lm in syn.lemmas()}\n",
    "\n",
    "def semantic_match(text, thresh=0.2):\n",
    "    v = vectorizer.transform([text])\n",
    "    sims = cosine_similarity(v, vectors).flatten()\n",
    "    idx  = sims.argmax()\n",
    "    return food_list[idx] if sims[idx] >= thresh else None\n",
    "\n",
    "def fuzzy_suggestions(text, n=3, cutoff=60):\n",
    "    scores = [(name, fuzz.token_set_ratio(text, name)) for name in food_list]\n",
    "    filtered = sorted([p for p in scores if p[1] >= cutoff], key=lambda x: -x[1])\n",
    "    return [name for name, score in filtered[:n]]\n",
    "\n",
    "def normalize(tok):\n",
    "    return lem.lemmatize(tok.lower(), pos='n')\n",
    "\n",
    "def autocorrect_text(text):\n",
    "    words = re.findall(r'\\w+', text)\n",
    "    corrected_words = [spell(word) for word in words]\n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "    if corrected_text != text:\n",
    "        console.print(f\"[yellow]Autocorrected input: '{text}' â†’ '{corrected_text}'[/yellow]\")\n",
    "    return corrected_text\n",
    "\n",
    "# === Similarity-based Suggestions ===\n",
    "def suggest_similar_foods(food_key, top_n=5):\n",
    "    if food_key not in food_list:\n",
    "        return []\n",
    "    idx = food_list.index(food_key)\n",
    "    sims = cosine_similarity(vectors[idx], vectors).flatten()\n",
    "    top_idx = np.argsort(sims)[-top_n-1:-1][::-1]\n",
    "    return [food_list[i] for i in top_idx]\n",
    "\n",
    "# === Extraction Logic ===\n",
    "def extract_single_food_and_qty(text):\n",
    "    txt = text.lower()\n",
    "    txt = autocorrect_text(txt)  # <-- Autocorrect here\n",
    "    qty = 100.0\n",
    "    m = re.search(r\"(\\d+(?:\\.\\d+)?)\\s*(g|gram|kg|oz)\", txt)\n",
    "    if m:\n",
    "        val, unit = float(m.group(1)), m.group(2)\n",
    "        qty = val * 1000 if 'kg' in unit else (val*28.35 if 'oz' in unit else val)\n",
    "    for name in food_list:\n",
    "        if name in txt:\n",
    "            return name, qty\n",
    "    suggestions = fuzzy_suggestions(txt)\n",
    "    if suggestions:\n",
    "        if len(suggestions) == 1:\n",
    "            return suggestions[0], qty\n",
    "        console.print(f\"[yellow]I found multiple close matches for '{text}':[/yellow]\")\n",
    "        for idx, opt in enumerate(suggestions, 1):\n",
    "            console.print(f\"  {idx}. {opt}\")\n",
    "        choice = console.input(\"Enter the number of the correct item, or 0 to skip: \")\n",
    "        console.print(f\"[blue]You:[/blue] {choice}\")\n",
    "        if choice.isdigit() and 1 <= int(choice) <= len(suggestions):\n",
    "            return suggestions[int(choice)-1], qty\n",
    "    sem = semantic_match(txt)\n",
    "    if sem:\n",
    "        console.print(f\"[yellow]Semantically matched '{text}' to '{sem}'[/yellow]\")\n",
    "        return sem, qty\n",
    "    for tok in re.findall(r\"\\w+\", txt):\n",
    "        for syn in get_synonyms(tok):\n",
    "            if syn in food_list:\n",
    "                console.print(f\"[yellow]Using synonym '{syn}' for '{tok}'[/yellow]\")\n",
    "                return syn, qty\n",
    "    return None, qty\n",
    "\n",
    "def extract_foods_and_qty(user_text):\n",
    "    clauses = re.split(r',| and ', user_text)\n",
    "    results = []\n",
    "    for clause in clauses:\n",
    "        name, qty = extract_single_food_and_qty(clause)\n",
    "        if name:\n",
    "            results.append((name, qty))\n",
    "    return results\n",
    "\n",
    "# === Prediction & Animated Response ===\n",
    "def predict_and_respond(food_key, qty):\n",
    "    record = food_map.get(food_key)\n",
    "    if not record:\n",
    "        console.print(f\"[red]Sorry, I couldn't find that food: {food_key}[/red]\")\n",
    "        return\n",
    "    nutrition = {k: v for k, v in record.items() if isinstance(v, (int, float))}\n",
    "    with console.status(\"Analyzing nutrition...\", spinner=\"bouncingBar\"):\n",
    "        time.sleep(1.5)\n",
    "        df_input = pd.DataFrame([{**nutrition, 'Food_Item': record['Food_Item']}])\n",
    "        X = preproc.transform(df_input)\n",
    "        probs = model.predict(X, verbose=0)[0]\n",
    "        idx = np.argmax(probs)\n",
    "        meal = label_enc.inverse_transform([idx])[0]\n",
    "        conf = probs[idx]\n",
    "    console.print(f\":plate_with_cutlery: [bold green]{qty:.0f}g {record['Food_Item']}[/bold green] \"\n",
    "                  f\"â†’ [cyan]{meal}[/cyan] ([magenta]{conf*100:.1f}%[/magenta])\")\n",
    "    console.print(\"[underline]Nutrition per serving:[/underline]\")\n",
    "    for k, v in nutrition.items():\n",
    "        console.print(f\"- {k}: [bold]{v * qty/100:.2f}[/bold]\")\n",
    "\n",
    "# === Main CLI Loop v21 ===\n",
    "def chatbot():\n",
    "    console.print(\":seedling: [bold]Welcome to Smart Food NutriBot v21[/bold] (type 'exit' to quit)\\n\")\n",
    "    while True:\n",
    "        user = console.input(\"[blue]You:[/blue] \").strip()\n",
    "        console.print(f\"[blue]You:[/blue] {user}\")\n",
    "        if user.lower() == 'exit':\n",
    "            console.print(f\"[bold yellow]{random.choice(FAREWELL_RESPONSES)}[/bold yellow]\")\n",
    "            break\n",
    "        if GREETINGS_PATTERN.search(user):\n",
    "            console.print(f\"[green]{random.choice(GREETING_RESPONSES)}[/green]\")\n",
    "            continue\n",
    "        if THANKS_PATTERN.search(user):\n",
    "            console.print(f\"[green]{random.choice(THANK_RESPONSES)}[/green]\")\n",
    "            continue\n",
    "        if FAREWELL_PATTERN.search(user):\n",
    "            console.print(f\"[green]{random.choice(FAREWELL_RESPONSES)}[/green]\")\n",
    "            continue\n",
    "        if re.search(r\"\\b(suggest|recommend)\\b\", user.lower()):\n",
    "            name, _ = extract_single_food_and_qty(user)\n",
    "            if name:\n",
    "                record = food_map.get(name)\n",
    "                sim_list = suggest_similar_foods(name)\n",
    "                if sim_list:\n",
    "                    console.print(f\":mag: Here are some foods similar to [bold]{record['Food_Item']}[/bold]:\")\n",
    "                    for item in sim_list:\n",
    "                        console.print(f\"- {food_map[item]['Food_Item']}\")\n",
    "                else:\n",
    "                    console.print(f\"[red]Sorry, I don't have suggestions for {record['Food_Item']}.[/red]\")\n",
    "                continue\n",
    "        foods = extract_foods_and_qty(user)\n",
    "        if not foods:\n",
    "            console.print(\"[red]I couldn't identify any food items in your query. Could you rephrase?[/red]\")\n",
    "            continue\n",
    "        for food_key, qty in foods:\n",
    "            predict_and_respond(food_key, qty)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chatbot()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "deepnote_notebook_id": "26f633766bc54911a312b048a19d6266",
  "kernelspec": {
   "display_name": "aidev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
